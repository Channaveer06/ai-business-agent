# src/utils/llm_client.py

"""
LLM client abstraction.

- FakeLLMClient: for offline testing, no real API.
- RealLLMClient: skeleton for integrating a real LLM provider (Gemini, OpenAI, etc.)

This design allows you to switch implementations without changing agent code.
"""

import logging
from textwrap import shorten
from typing import Optional

from config import LLM_PROVIDER, LLM_API_KEY

logger = logging.getLogger(__name__)


class FakeLLMClient:
    """
    Very simple fake LLM for offline testing.

    It does NOT generate real language, but:
    - Logs the prompt
    - Returns a stub response indicating what it would have done
    """

    def generate(self, prompt: str, max_tokens: int = 512) -> str:
        logger.info(
            "FakeLLMClient.generate called with prompt (first 120 chars): %s",
            shorten(prompt, width=120, placeholder="..."),
        )

        return (
            "FAKE LLM RESPONSE\n"
            "-----------------\n"
            "This is a placeholder response generated by FakeLLMClient.\n\n"
            "In a real deployment, this would be replaced by a call to an LLM\n"
            f"(provider={LLM_PROVIDER}) using the prompt that was passed in.\n"
        )


class RealLLMClient:
    """
    Skeleton for a real LLM client (e.g., Gemini, OpenAI).

    For safety and to avoid dependency on external APIs, this class
    is left as a placeholder. You can implement it using the official
    SDK of your chosen provider (Gemini, OpenAI, etc.).
    """

    def __init__(self, api_key: Optional[str] = None, provider: Optional[str] = None):
        self.api_key = api_key or LLM_API_KEY
        self.provider = provider or LLM_PROVIDER

        if self.api_key is None:
            raise ValueError(
                "No API key provided for RealLLMClient. "
                "Set LLM_API_KEY in your environment or .env file."
            )

        logger.info("Initialized RealLLMClient with provider=%s", self.provider)

        # NOTE:
        # This is where you would initialize the real SDK client, e.g.:
        #
        # if self.provider == "gemini":
        #     import google.generativeai as genai
        #     genai.configure(api_key=self.api_key)
        #     self.client = genai.GenerativeModel("gemini-1.5-pro")
        #
        # elif self.provider == "openai":
        #     from openai import OpenAI
        #     self.client = OpenAI(api_key=self.api_key)
        #
        # For this capstone, we leave it commented to avoid external dependencies.

    def generate(self, prompt: str, max_tokens: int = 512) -> str:
        """
        Call the underlying real LLM.

        This is a placeholder implementation; you must fill it based
        on your chosen provider's SDK.
        """
        raise NotImplementedError(
            "RealLLMClient.generate is not implemented. "
            "Implement it using your LLM provider's SDK."
        )
