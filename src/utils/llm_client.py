# src/utils/llm_client.py

"""
LLM client abstraction.

- FakeLLMClient: for offline testing, no real API.
- RealLLMClient: uses Gemini (via google-generativeai) when configured.

Agents (EmailAgent, MeetingAgent) can choose Fake or Real based on config.
"""

import logging
from textwrap import shorten
from typing import Optional

from config import LLM_PROVIDER, LLM_API_KEY

logger = logging.getLogger(__name__)

# Try to import google.generativeai for Gemini, but don't crash if missing.
try:
    import google.generativeai as genai
    _HAS_GEMINI = True
except ImportError:
    _HAS_GEMINI = False
    genai = None  # type: ignore


class FakeLLMClient:
    """
    Simple fake LLM for offline testing.

    It does NOT generate real language, but:
    - Logs the prompt
    - Returns a stub response indicating what it would have done
    """

    def generate(self, prompt: str, max_tokens: int = 512) -> str:
        logger.info(
            "FakeLLMClient.generate called with prompt (first 120 chars): %s",
            shorten(prompt, width=120, placeholder="..."),
        )

        return (
            "FAKE LLM RESPONSE\n"
            "-----------------\n"
            "This is a placeholder response generated by FakeLLMClient.\n\n"
            "In a real deployment, this would be replaced by a call to an LLM\n"
            f"(provider={LLM_PROVIDER}) using the prompt that was passed in.\n"
        )


class RealLLMClient:
    """
    Real LLM client using Gemini (google-generativeai).

    Reads:
    - LLM_PROVIDER from config (should be 'gemini')
    - LLM_API_KEY from config or passed explicitly
    """

    def __init__(
        self,
        api_key: Optional[str] = None,
        provider: Optional[str] = None,
        model_name: str = "models/gemini-flash-latest",
    ):
        self.provider = provider or LLM_PROVIDER
        self.api_key = api_key or LLM_API_KEY
        self.model_name = model_name

        if self.provider != "gemini":
            raise ValueError(
                f"RealLLMClient currently only supports provider='gemini', "
                f"but got provider='{self.provider}'"
            )

        if not self.api_key:
            raise ValueError(
                "No API key provided for RealLLMClient. "
                "Set LLM_API_KEY in your .env or environment."
            )

        if not _HAS_GEMINI:
            raise ImportError(
                "google-generativeai is not installed. "
                "Install it with: pip install google-generativeai"
            )

        # Configure Gemini
        genai.configure(api_key=self.api_key)
        self.client = genai.GenerativeModel(self.model_name)

        logger.info(
            "Initialized RealLLMClient with provider=%s, model=%s",
            self.provider,
            self.model_name,
        )

    def generate(self, prompt: str, max_tokens: int = 512) -> str:
        """
        Call the underlying Gemini model.

        If anything goes wrong, we log and return an error message
        instead of crashing the whole app.
        """
        logger.info(
            "RealLLMClient.generate called (first 120 chars of prompt): %s",
            shorten(prompt, width=120, placeholder="..."),
        )

        try:
            response = self.client.generate_content(prompt)
            # response.text is usually the main text output
            text = getattr(response, "text", None)
            if not text:
                logger.warning("Gemini response had no 'text' attribute or was empty.")
                return "[RealLLMClient] Empty response from Gemini."
            return text
        except Exception as e:
            logger.exception("Error while calling Gemini LLM")
            return f"[RealLLMClient] Error while calling LLM: {e}"
